import requests
import psycopg2
import time
import re
from datetime import datetime, timedelta
from config import DB_CONFIG, KEYWORDS, GEO_CONFIG

class HHParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'HH-Parser/1.0',
            'Accept': 'application/json'
        })
    
    def clean_text(self, text):
        """–ü—Ä–æ—Å—Ç–∞—è –∏ –Ω–∞–¥–µ–∂–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ encode/decode"""
        if not text:
            return ""
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–∞–π—Ç—ã –∏ –æ–±—Ä–∞—Ç–Ω–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ—à–∏–±–∫–∏
            cleaned_text = text.encode('utf-8', 'ignore').decode('utf-8')
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–±–∏—Ä–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            cleaned_text = re.sub(r'[^\w\s\.\,\-\+\!\?\:\;\(\)\"\']', '', cleaned_text)
            
            return cleaned_text.strip()
            
        except:
            return ""
    
    def clean_html_tags(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤ - —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –±–∏—Ç—ã—Ö —Ç–µ–≥–æ–≤"""
        if not text:
            return ""
        
        # –®–∞–≥ 1: –ó–∞–º–µ–Ω—è–µ–º "–±–∏—Ç—ã–µ" —Ç–µ–≥–∏ –≤—Ä–æ–¥–µ li> –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ <li>
        text = re.sub(r'(\w+)>', r'<\1>', text)
        
        # –®–∞–≥ 2: –£–¥–∞–ª—è–µ–º –í–°–ï HTML —Ç–µ–≥–∏
        clean_text = re.sub(r'</?[^>]*>', '', text)
        
        # –®–∞–≥ 3: –ó–∞–º–µ–Ω—è–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏
        html_entities = {
            '&nbsp;': ' ', '&amp;': '&', '&lt;': '<', 
            '&gt;': '>', '&quot;': '"', '&apos;': "'"
        }
        for entity, replacement in html_entities.items():
            clean_text = clean_text.replace(entity, replacement)
        
        # –®–∞–≥ 4: –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        clean_text = re.sub(r'\s+', ' ', clean_text)
        
        return clean_text.strip()
    
    def extract_skills_from_text(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return "–Ω–µ —É–∫–∞–∑–∞–Ω—ã"
        
        text_lower = text.lower()
        tech_keywords = [
            'python', 'sql', 'excel', 'n8n', 'make.com', 'zapier', 'airflow', 
            'power bi', 'tableau', 'superset', 'metabase', 'vba', 'pandas',
            'numpy', 'etl', 'api', 'docker', 'git', 'postgresql', 'mysql',
            'selenium', 'postman', 'jira', 'confluence'
        ]
        
        found_skills = []
        for keyword in tech_keywords:
            if keyword in text_lower:
                found_skills.append(keyword)
        
        return ', '.join(found_skills) if found_skills else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'
    
    def check_work_format(self, vacancy_item, vacancy_detail):
        """–î–ï–ë–ê–ì –í–ï–†–°–ò–Ø - –ø–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –≤ API"""
        schedule = vacancy_item.get('schedule', {})
        schedule_id = schedule.get('id')
        schedule_name = schedule.get('name', '')
        
        print(f"üéØ –î–ï–ë–ê–ì –§–æ—Ä–º–∞—Ç–∞: {vacancy_item['name'][:30]}...")
        print(f"   üìÖ Schedule ID: '{schedule_id}'")
        print(f"   üìÖ Schedule Name: '{schedule_name}'")
        
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ - –±–µ—Ä–µ–º –¢–û–õ–¨–ö–û –∏–∑ schedule.id
        if schedule_id == 'remote':
            result = 'remote'
        elif schedule_id in ['fullDay', 'shift', 'flexible']:
            result = 'office'
        else:
            result = 'unknown'
        
        print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        print("-" * 40)
        
        return result
    
    def is_suitable_location(self, work_format, area_id):
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ç–µ—Å—Ç–∞"""
        is_tyumen = area_id == GEO_CONFIG['preferred_city_id']
        
        print(f"üìç –ì–µ–æ-—Ñ–∏–ª—å—Ç—Ä: –¢—é–º–µ–Ω—å={is_tyumen}, –§–æ—Ä–º–∞—Ç={work_format}")
        
        # –¢—é–º–µ–Ω—å - –í–°–ï —Ñ–æ—Ä–º–∞—Ç—ã
        if is_tyumen:
            print("   ‚úÖ –¢—é–º–µ–Ω—å - –ø–æ–¥—Ö–æ–¥–∏—Ç –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç")
            return True
        
        # –î—Ä—É–≥–∏–µ –≥–æ—Ä–æ–¥–∞ - —Ç–æ–ª—å–∫–æ —É–¥–∞–ª–µ–Ω–∫–∞
        if work_format == 'remote':
            print("   ‚úÖ –î—Ä—É–≥–æ–π –≥–æ—Ä–æ–¥ - –ø–æ–¥—Ö–æ–¥–∏—Ç remote")
            return True
        else:
            print(f"   ‚ùå –î—Ä—É–≥–æ–π –≥–æ—Ä–æ–¥ - –ù–ï –ø–æ–¥—Ö–æ–¥–∏—Ç {work_format}")
            return False
        
    def parse_salary(self, salary_data):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞—Ä–ø–ª–∞—Ç—ã"""
        if not salary_data:
            return None, None
        
        salary_from = salary_data.get('from')
        salary_to = salary_data.get('to')
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ä—É–±–ª–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤ –¥—Ä—É–≥–æ–π –≤–∞–ª—é—Ç–µ
        if salary_data.get('currency') != 'RUR':
            if salary_from:
                salary_from = salary_from * 70
            if salary_to:
                salary_to = salary_to * 70
        
        return salary_from, salary_to
    
    def categorize_vacancy(self, vacancy):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –≤–∞–∫–∞–Ω—Å–∏–π"""
        title = vacancy['name'].lower()
        desc = vacancy.get('description', '').lower()
        skills = vacancy.get('skills', '').lower()
        
        text = f"{title} {desc} {skills}"
        
        categories_priority = [
            {
                'name': 'automation',
                'keywords': ['n8n', 'make.com', 'zapier', 'rpa', 'workflow', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü']
            },
            {
                'name': 'data_engineering', 
                'keywords': ['airflow', 'etl', 'data pipeline', 'dbt', 'data engineer', 'data engineering']
            },
            {
                'name': 'bi',
                'keywords': ['power bi', 'tableau', 'superset', 'metabase', 'bi analyst', '–¥–∞—à–±–æ—Ä–¥', 'dashboard']
            },
            {
                'name': 'python_sql',
                'keywords': ['python', 'sql', 'pandas', 'numpy', 'data analysis', 'scripting']
            },
            {
                'name': 'excel',
                'keywords': [
                    'excel', 'vba', '–º–∞–∫—Ä–æ—Å', 'power query', '—Å–≤–æ–¥–Ω', 'macro',
                    'pivot table', 'pivot', '—Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞', '—Å–≤–æ–¥–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã',
                    'vlookup', '–≤–ø—Ä', 'xlookup', 'index match', '–ø–æ–∏—Å–∫–ø—Ä–∞–≤',
                    'power pivot', 'get transform', '—Ñ–∏–ª—å—Ç—Ä—ã', 'advanced filter',
                    '—É—Å–ª–æ–≤–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', 'conditional formatting',
                    '–¥–∏–∞–≥—Ä–∞–º–º–∞', 'chart', '–≥—Ä–∞—Ñ–∏–∫', 'sparkline',
                    'data analysis', '–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö', 'excel –∞–Ω–∞–ª–∏—Ç–∏–∫',
                    '—Ñ–æ—Ä–º—É–ª—ã'
                ]
            },
            {
                'name': 'documentation',
                'keywords': [
                    'confluence', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫', '–∏–Ω—Å—Ç—Ä—É–∫—Ü', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤',
                    'technical writer', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è',
                    '–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π', 'knowledge base', '–º–∞–Ω—É–∞–ª', 'manual', 
                    '—Ç–µ—Ö–ø–∏—Å–∞—Ç–µ–ª—å', '—Ç–µ—Ö –ø–∏—Å–∞—Ç–µ–ª—å', 'write documentation'
                ]
            },
            {
                'name': 'qa_testing',
                'keywords': [
                    'qa', 'quality assurance', '—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', 'testing',
                    'manual testing', '—Ä—É—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è',
                    'test automation', 'selenium', 'postman', 'api testing', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ api',
                    'functional testing', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', 'regression testing',
                    '—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', 'test case', '—Ç–µ—Å—Ç –∫–µ–π—Å', 'test plan'
                    , 'bug report', '–±–∞–≥ —Ä–µ–ø–æ—Ä—Ç',  'jira', 'testrail',
                    'qc', 'quality control', '–∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞', 'junior qa', 'trainee qa',
                    'qa engineer', '–∏–Ω–∂–µ–Ω–µ—Ä –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é', 'software tester'
                ]
            },
            {
                'name':'archivist', 
                'keywords': ['–∞—Ä—Ö–∏–≤–∞—Ä–∏—É—Å', '–∞—Ä—Ö–∏–≤', '–¥–µ–ª–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–¥–µ–ª–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–∫–∞–Ω—Ü–µ–ª—è—Ä–∏—è']
            }
        ]
        
        for category in categories_priority:
            for keyword in category['keywords']:
                if keyword in text:
                    return category['name']
        
        return 'other'
    
    def calculate_relevance(self, vacancy, work_format):
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –ª–æ–∫–∞—Ü–∏–∏"""
        score = 0
        title = vacancy['name'].lower()
        desc = vacancy.get('description', '').lower()
        skills = vacancy.get('skills', '').lower()
        
        text = f"{title} {desc} {skills}"
        
        high_priority = ['n8n', 'airflow', 'superset', 'power bi', 'tableau', 'etl', 'dbt']
        medium_priority = ['python', 'sql', 'pandas', 'vba', 'excel', 'dashboard']
        low_priority = ['data', 'analysis', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü', '–∞–Ω–∞–ª–∏–∑']
        
        for keyword in high_priority:
            if keyword in text:
                score += 3
        
        for keyword in medium_priority:
            if keyword in text:
                score += 2
                
        for keyword in low_priority:
            if keyword in text:
                score += 1
        
        # –ë–æ–Ω—É—Å –∑–∞ —É–∫–∞–∑–∞–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã
        if vacancy.get('salary_from') or vacancy.get('salary_to'):
            score += 2
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã
        if work_format == 'remote':
            score += 3
        elif work_format == 'hybrid':
            score += 2
        elif work_format == 'office':
            score += 1
            
        return min(score, 10)
    
    def get_vacancy_details(self, vacancy_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        try:
            response = self.session.get(
                f"https://api.hh.ru/vacancies/{vacancy_id}",
                timeout=10
            )
            if response.status_code == 200:
                return response.json()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}: {e}")
        return {}
    
    def get_hh_vacancies(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π - –†–ê–ó–î–ï–õ–¨–ù–´–ô –ø–æ–∏—Å–∫"""
        url = "https://api.hh.ru/vacancies"
        all_vacancies = []
        
        # 1. –°–ù–ê–ß–ê–õ–ê –ò–©–ï–ú –ü–û –¢–Æ–ú–ï–ù–ò (–≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ä–∞–±–æ—Ç—ã)
        print("üéØ –≠–¢–ê–ü 1: –ü–û–ò–°–ö –ü–û –¢–Æ–ú–ï–ù–ò (–æ—Ñ–∏—Å/–≥–∏–±—Ä–∏–¥/—É–¥–∞–ª–µ–Ω–∫–∞)")
        print("=" * 50)
        
        for keyword in KEYWORDS:
            print(f"üîç –¢—é–º–µ–Ω—å: {keyword}")
            
            page = 0
            total_pages = 1
            
            while page < total_pages:
                params = {
                    'text': keyword,
                    'area': 1410,  # –¢–æ–ª—å–∫–æ –¢—é–º–µ–Ω—å!
                    'per_page': 20,
                    'page': page,
                    'order_by': 'publication_time',
                }
                
                try:
                    response = self.session.get(url, params=params, timeout=10)
                    data = response.json()
                    
                    total_pages = min(data.get('pages', 1), 2)  # –ú–∞–∫—Å–∏–º—É–º 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    found = data.get('found', 0)
                    
                    if page == 0:
                        print(f"   üì® –ù–∞–π–¥–µ–Ω–æ –≤ –¢—é–º–µ–Ω–∏: {found}")
                        print(f"   üìÑ –°—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_pages}")
                    
                    print(f"   üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}/{total_pages}")
                    
                    page_vacancies = 0
                    for item in data.get('items', []):
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
                        published = datetime.fromisoformat(item['published_at'].replace('Z', '+00:00'))
                        if datetime.now(published.tzinfo) - published > timedelta(days=10):
                            continue
                        
                        # –§–ò–õ–¨–¢–† –ü–û –û–ü–´–¢–£
                        experience = item.get('experience', {})
                        experience_id = experience.get('id')
                        if experience_id not in ['noExperience', 'between1And3']:
                            continue
                        
                        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏
                        vacancy_detail = self.get_vacancy_details(item['id'])
                        time.sleep(0.1)
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã
                        work_format = self.check_work_format(item, vacancy_detail)
                        
                        # –î–ª—è –¢—é–º–µ–Ω–∏ –í–°–ï —Ñ–æ—Ä–º–∞—Ç—ã –ø–æ–¥—Ö–æ–¥—è—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                        salary_from, salary_to = self.parse_salary(item.get('salary'))
                        
                        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê SKILLS –ò DESCRIPTION
                        skills_from_api = [s['name'] for s in item.get('key_skills', [])]
                        if skills_from_api:
                            cleaned_skills = ', '.join(skills_from_api)
                        else:
                            # –ï—Å–ª–∏ API –Ω–µ –¥–∞–ª–æ skills, –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                            cleaned_skills = self.extract_skills_from_text(vacancy_detail.get('description', ''))
                        
                        # –û—á–∏—â–∞–µ–º description –æ—Ç HTML —Ç–µ–≥–æ–≤
                        raw_description = vacancy_detail.get('description', '')
                        cleaned_description = self.clean_html_tags(raw_description)[:1500]
                        
                        vacancy = {
                            'hh_id': int(item['id']),
                            'name': self.clean_text(item['name']),
                            'company': self.clean_text(item['employer']['name']),
                            'salary_from': salary_from,
                            'salary_to': salary_to,
                            'url': item['alternate_url'],
                            'skills': cleaned_skills,
                            'description': cleaned_description,
                            'work_format': work_format,
                            'city': item.get('area', {}).get('name', '–¢—é–º–µ–Ω—å')
                        }
                        
                        vacancy['category'] = self.categorize_vacancy(vacancy)
                        vacancy['relevance_score'] = self.calculate_relevance(vacancy, work_format)
                        
                        all_vacancies.append(vacancy)
                        page_vacancies += 1
                        print(f"   ‚úÖ –¢—é–º–µ–Ω—å: {vacancy['name'][:40]}... | {work_format}")
                    
                    print(f"   üìä –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1} –¥–æ–±–∞–≤–ª–µ–Ω–æ: {page_vacancies} –≤–∞–∫–∞–Ω—Å–∏–π")
                    page += 1
                    time.sleep(0.3)
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –¢—é–º–µ–Ω–∏ {keyword}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {e}")
                    break
        
        # 2. –ü–û–¢–û–ú –ò–©–ï–ú –ü–û –†–û–°–°–ò–ò (—Ç–æ–ª—å–∫–æ —É–¥–∞–ª–µ–Ω–∫–∞, –±–µ–∑ –¢—é–º–µ–Ω–∏)
        print("\nüéØ –≠–¢–ê–ü 2: –ü–û–ò–°–ö –ü–û –†–û–°–°–ò–ò (—Ç–æ–ª—å–∫–æ —É–¥–∞–ª–µ–Ω–∫–∞, –±–µ–∑ –¢—é–º–µ–Ω–∏)")
        print("=" * 50)
        
        for keyword in KEYWORDS:
            print(f"üîç –†–æ—Å—Å–∏—è (–±–µ–∑ –¢—é–º–µ–Ω–∏): {keyword}")
            
            page = 0
            total_pages = 1
            
            while page < total_pages:
                params = {
                    'text': f'{keyword} !–¢—é–º–µ–Ω—å',  # ‚ùå –ò—Å–∫–ª—é—á–∞–µ–º –¢—é–º–µ–Ω—å
                    'area': 113,  # –í—Å—è –†–æ—Å—Å–∏—è
                    'per_page': 30,
                    'page': page,
                    'order_by': 'publication_time',
                }
                
                try:
                    response = self.session.get(url, params=params, timeout=10)
                    data = response.json()
                    
                    total_pages = min(data.get('pages', 1), 2)  # –ú–∞–∫—Å–∏–º—É–º 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    found = data.get('found', 0)
                    
                    if page == 0:
                        print(f"   üì® –ù–∞–π–¥–µ–Ω–æ –≤ –†–æ—Å—Å–∏–∏ (–±–µ–∑ –¢—é–º–µ–Ω–∏): {found}")
                        print(f"   üìÑ –°—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_pages}")
                    
                    print(f"   üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}/{total_pages}")
                    
                    page_vacancies = 0
                    for item in data.get('items', []):
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
                        published = datetime.fromisoformat(item['published_at'].replace('Z', '+00:00'))
                        if datetime.now(published.tzinfo) - published > timedelta(days=5):
                            continue
                        
                        # –§–ò–õ–¨–¢–† –ü–û –û–ü–´–¢–£
                        experience = item.get('experience', {})
                        experience_id = experience.get('id')
                        if experience_id not in ['noExperience', 'between1And3']:
                            continue
                        
                        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏
                        vacancy_detail = self.get_vacancy_details(item['id'])
                        time.sleep(0.1)
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã
                        work_format = self.check_work_format(item, vacancy_detail)
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ª–æ–∫–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ —É–¥–∞–ª–µ–Ω–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –≥–æ—Ä–æ–¥–æ–≤)
                        if work_format != 'remote':
                            continue
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —É–¥–∞–ª–µ–Ω–∫—É
                        salary_from, salary_to = self.parse_salary(item.get('salary'))
                        
                        # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê SKILLS –ò DESCRIPTION
                        skills_from_api = [s['name'] for s in item.get('key_skills', [])]
                        if skills_from_api:
                            cleaned_skills = ', '.join(skills_from_api)
                        else:
                            cleaned_skills = self.extract_skills_from_text(vacancy_detail.get('description', ''))
                        
                        # –û—á–∏—â–∞–µ–º description –æ—Ç HTML —Ç–µ–≥–æ–≤
                        raw_description = vacancy_detail.get('description', '')
                        cleaned_description = self.clean_html_tags(raw_description)[:1500]
                        
                        vacancy = {
                            'hh_id': int(item['id']),
                            'name': self.clean_text(item['name']),
                            'company': self.clean_text(item['employer']['name']),
                            'salary_from': salary_from,
                            'salary_to': salary_to,
                            'url': item['alternate_url'],
                            'skills': cleaned_skills,
                            'description': cleaned_description,
                            'work_format': work_format,
                            'city': item.get('area', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω')
                        }
                        
                        vacancy['category'] = self.categorize_vacancy(vacancy)
                        vacancy['relevance_score'] = self.calculate_relevance(vacancy, work_format)
                        
                        all_vacancies.append(vacancy)
                        page_vacancies += 1
                        print(f"   ‚úÖ –†–æ—Å—Å–∏—è: {vacancy['name'][:40]}... | {work_format}")
                    
                    print(f"   üìä –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1} –¥–æ–±–∞–≤–ª–µ–Ω–æ: {page_vacancies} –≤–∞–∫–∞–Ω—Å–∏–π")
                    page += 1
                    time.sleep(0.3)
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –†–æ—Å—Å–∏–∏ {keyword}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {e}")
                    break
        
        return all_vacancies

def save_to_db(vacancies):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    new_count = 0
    duplicate_count = 0
    error_count = 0
    
    for vac in vacancies:
        try:
            cursor.execute("""
                INSERT INTO vacancies 
                (hh_id, name, company, salary_from, salary_to, url, skills, 
                 description, category, relevance_score, work_format, city)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (hh_id) DO NOTHING
                RETURNING id
            """, (
                vac['hh_id'], vac['name'], vac['company'], vac['salary_from'], 
                vac['salary_to'], vac['url'], vac['skills'], vac['description'],
                vac['category'], vac['relevance_score'], vac['work_format'], vac['city']
            ))
            
            if cursor.fetchone():
                new_count += 1
            else:
                duplicate_count += 1
                
        except Exception as e:
            error_count += 1
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vac['hh_id']}: {e}")
    
    conn.commit()
    cursor.close()
    conn.close()
    
    return new_count, duplicate_count, error_count

if __name__ == "__main__":
    print(f"üïí {datetime.now()} - –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ HH.ru")
    print(f"üìç –ì–µ–æ-—Ñ–∏–ª—å—Ç—Ä: –¢—é–º–µ–Ω—å - –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç, –¥—Ä—É–≥–∏–µ –≥–æ—Ä–æ–¥–∞ - —Ç–æ–ª—å–∫–æ —É–¥–∞–ª–µ–Ω–∫–∞")
    print(f"üíº –û–ø—ã—Ç: –±–µ–∑ –æ–ø—ã—Ç–∞ –∏–ª–∏ 1-3 –≥–æ–¥–∞")
    print("=" * 60)
    
    parser = HHParser()
    vacancies = parser.get_hh_vacancies()
    
    print(f"üéØ –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(vacancies)}")
    
    if vacancies:
        new_count, duplicate_count, error_count = save_to_db(vacancies)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: –Ω–æ–≤—ã—Ö {new_count}, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ {duplicate_count}, –æ—à–∏–±–æ–∫ {error_count}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º —Ä–∞–±–æ—Ç—ã
        formats = {}
        for vac in vacancies:
            fmt = vac['work_format']
            formats[fmt] = formats.get(fmt, 0) + 1
        
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º —Ä–∞–±–æ—Ç—ã:")
        for fmt, count in formats.items():
            print(f"  {fmt}: {count} –≤–∞–∫–∞–Ω—Å–∏–π")
    else:
        print("‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")